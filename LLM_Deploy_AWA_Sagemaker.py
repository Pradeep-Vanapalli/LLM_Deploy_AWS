# -*- coding: utf-8 -*-
"""Welcome To Colaboratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

!pip install transformers einops accelerate bitsandbytes

from transformers import AutoTokenizer AutoModelForSeqtoSeqLM
from transformers import pipeline
import torch
import base64

checkpoint = "MBZUAI/LaMini-Flan-T5-783M"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
base_model = AutoModelForSeqtoSeqLM.from_pretrained(checkpoint, device_map="auto", torch_dtype = torch.float32)

!pip install langchain

from langchain.llms import HuggingFacePipeline

def llm_pipeline():
  pipe = pipeline(
      'text2text-generation',
      model = base_model,
      tokenizer = tokenizer,
      max_length = 256,
      do_sample = True,
      temperature = 0.3,
      top_p = 0.95
  )
  local_llm = HuggingFacePipeline(pipeline=pipe)
  return local_llm

input_prompt = "Write an article of Artificial Intelligence"

model = llm_pipeline()
generated_text = model(input_prompt)
generated_text

!pip uninstall -y sagemaker

!pip install sagemaker==2.168.0

import json
import sagemaker
import boto3
from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri

try:
	role = sagemaker.get_execution_role()
except ValueError:
	iam = boto3.client('iam')
	role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']

# Hub Model configuration. https://huggingface.co/models
hub = {
	'HF_MODEL_ID':'MBZUAI/LaMini-Flan-T5-783M',
  'HF_TASK':'text2text-generation',
  'device_map':'auto',
  'torch_dtype':'torch.float.32'
	}


# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
	image_uri=get_huggingface_llm_image_uri("huggingface",version="1.1.0"),
	env=hub,
	role=role,
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
	initial_instance_count=1,
	instance_type="ml.g4dn.2xlarge",
	container_startup_health_check_timeout=300,
  )

# send request
predictor.predict({
	"inputs": "Write a short article on Artificial Intelligence",
})

prompt = "Write a short article on Blockchain"

#hyperparameter payload
payload = {
    'inputs':prompt,
    'parameters':{
        'do_sample':True,
        'top_p':0.7,
        'temperature':0.3,
        'top_k':50,
        'max_new_tokens':512,
        'repetition_penalty':1.03
    }
}

# send request to the endpoint
response = predictor.predict(payload)
print(response  )

ENDPOINT = 'XXXXXX XXX'

import boto3

runtime = boto3.client('runtime.sagemaker')

response = runtime.invoke_endpoint(EndpointName=ENDPOINT, ContentType='application/json', Body=json.dumps(payload))

print(response)

prediction = json.loads(response['Body'].read().decode('utf-8'))

prediction[0]['generated_text']